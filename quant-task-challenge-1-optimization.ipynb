{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13125450,"sourceType":"datasetVersion","datasetId":8314705}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:27:09.899315Z","iopub.execute_input":"2025-09-21T12:27:09.899668Z","iopub.status.idle":"2025-09-21T12:27:10.285699Z","shell.execute_reply.started":"2025-09-21T12:27:09.899641Z","shell.execute_reply":"2025-09-21T12:27:10.284505Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasetsdatasets/train (1).csv\n/kaggle/input/datasetsdatasets/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nimport warnings\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load file paths\nfile_path_train = \"/kaggle/input/datasetsdatasets/train (1).csv\"\nfile_path_test = \"/kaggle/input/datasetsdatasets/test.csv\"\n\n# Load training data\ntry:\n    df_train = pd.read_csv(file_path_train)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"File not found at {file_path_train}\")\n\n# Load test data\ntry:\n    df_test = pd.read_csv(file_path_test)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"File not found at {file_path_test}\")\n\n# Define features and targets\nfeatures = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\"]\ntargets = [\"Y1\", \"Y2\"]\n\n# Verify columns exist in training data\nrequired_columns = features + targets\nmissing_cols_train = [col for col in required_columns if col not in df_train.columns]\nif missing_cols_train:\n    raise ValueError(f\"Missing columns in training dataset: {missing_cols_train}\")\n\n# Verify columns in test data (allow missing targets)\nmissing_cols_test = [col for col in features if col not in df_test.columns]\nif missing_cols_test:\n    raise ValueError(f\"Missing columns in test dataset: {missing_cols_test}\")\n\n# Check data types\nprint(\"Training DataFrame dtypes:\\n\", df_train.dtypes)\nprint(\"Test DataFrame dtypes:\\n\", df_test.dtypes)\nif not all(df_train[features].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in training features: {df_train[features].dtypes}\")\nif not all(df_test[features].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in test features: {df_test[features].dtypes}\")\nif all(col in df_train.columns for col in targets) and not all(df_train[targets].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in training targets: {df_train[targets].dtypes}\")\n\n# Check for NaNs in original datasets\nprint(\"NaNs in original training DataFrame:\", df_train.isna().sum().sum())\nprint(\"NaN counts per column in training DataFrame:\\n\", df_train.isna().sum())\nprint(\"NaNs in original test DataFrame:\", df_test.isna().sum().sum())\nprint(\"NaN counts per column in test DataFrame:\\n\", df_test.isna().sum())\n\n# Forward fill and backward fill NaNs in original datasets\ndf_train = df_train[features + targets].ffill().bfill()\ndf_test = df_test[features + (targets if all(col in df_test.columns for col in targets) else [])].ffill().bfill()\nprint(\"NaNs after ffill/bfill in training DataFrame:\", df_train.isna().sum().sum())\nprint(\"NaNs after ffill/bfill in test DataFrame:\", df_test.isna().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:27:10.287335Z","iopub.execute_input":"2025-09-21T12:27:10.287848Z","iopub.status.idle":"2025-09-21T12:27:11.681001Z","shell.execute_reply.started":"2025-09-21T12:27:10.287822Z","shell.execute_reply":"2025-09-21T12:27:11.679821Z"}},"outputs":[{"name":"stdout","text":"Training DataFrame dtypes:\n time      int64\nA       float64\nB       float64\nC       float64\nD       float64\nE       float64\nF       float64\nG       float64\nH       float64\nI       float64\nJ       float64\nK       float64\nL       float64\nM       float64\nN       float64\nY1      float64\nY2      float64\ndtype: object\nTest DataFrame dtypes:\n id        int64\ntime      int64\nA       float64\nB       float64\nC       float64\nD       float64\nE       float64\nF       float64\nG       float64\nH       float64\nI       float64\nJ       float64\nK       float64\nL       float64\nM       float64\nN       float64\ndtype: object\nNaNs in original training DataFrame: 0\nNaN counts per column in training DataFrame:\n time    0\nA       0\nB       0\nC       0\nD       0\nE       0\nF       0\nG       0\nH       0\nI       0\nJ       0\nK       0\nL       0\nM       0\nN       0\nY1      0\nY2      0\ndtype: int64\nNaNs in original test DataFrame: 0\nNaN counts per column in test DataFrame:\n id      0\ntime    0\nA       0\nB       0\nC       0\nD       0\nE       0\nF       0\nG       0\nH       0\nI       0\nJ       0\nK       0\nL       0\nM       0\nN       0\ndtype: int64\nNaNs after ffill/bfill in training DataFrame: 0\nNaNs after ffill/bfill in test DataFrame: 0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 1. Lag and Shift Features\ndef create_lag_features(dframe, feature_vars, lags=[1, 3, 7]):\n    df_lagged = dframe.copy()\n    new_columns = {}\n    for var in feature_vars:\n        for lag in lags:\n            new_columns[f'{var}_lag{lag}'] = df_lagged[var].shift(lag)\n    new_columns_df = pd.DataFrame(new_columns, index=df_lagged.index)\n    df_lagged = pd.concat([df_lagged, new_columns_df], axis=1)\n    lag_columns = list(new_columns.keys())\n    print(\"NaNs in lagged columns before ffill:\", df_lagged[lag_columns].isna().sum().sum())\n    df_lagged[lag_columns] = df_lagged[lag_columns].ffill().bfill()\n    print(\"NaNs in lagged columns after ffill/bfill:\", df_lagged[lag_columns].isna().sum().sum())\n    return df_lagged\n\n# 2. Rolling Window Statistics\ndef create_rolling_features(dframe, feature_vars, windows=[1, 3, 7], ema_span=7):\n    df_metrics = dframe.copy()\n    new_columns = {}\n    for var in feature_vars:\n        for window in windows:\n            new_columns[f'{var}_mean_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).mean()\n            new_columns[f'{var}_var_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).var().fillna(0)\n            new_columns[f'{var}_std_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).std()\n            new_columns[f'{var}_min_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).min()\n            new_columns[f'{var}_max_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).max()\n            new_columns[f'{var}_median_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).median()\n        new_columns[f'{var}_ema_{ema_span}d'] = df_metrics[var].ewm(span=ema_span, adjust=False).mean()\n    new_columns_df = pd.DataFrame(new_columns, index=df_metrics.index)\n    df_metrics = pd.concat([df_metrics, new_columns_df], axis=1)\n    rolling_columns = list(new_columns.keys())\n    print(\"NaNs in rolling columns before ffill:\", df_metrics[rolling_columns].isna().sum().sum())\n    df_metrics[rolling_columns] = df_metrics[rolling_columns].ffill().bfill()\n    print(\"NaNs in rolling columns after ffill/bfill:\", df_metrics[rolling_columns].isna().sum().sum())\n    return df_metrics\n\n# 3. Seasonal and Periodic features \ndef create_periodic_features(dframe, windows = [1, 3, 7]):\n    df_metrics = dframe.copy()\n    t = np.arange(len(dframe))\n    new_columns = {}\n    \n    for window in windows:\n        new_columns[f'sin_{window}d'] = np.sin(2 * np.pi * t / window)\n        new_columns[f'cos_{window}d'] = np.cos(2 * np.pi * t / window)\n    new_columns_df = pd.DataFrame(new_columns, index = dframe.index)\n    df_metrics = pd.concat([df_metrics, new_columns_df], axis = 1)\n\n    return df_metrics\n\n# 4. Multivariate features \ndef multivariate_features(dframe, feature_vars):\n\n    if not isinstance(dframe, pd.DataFrame):\n        raise ValueError(\"Input 'dframe' must be a pandas DataFrame\")\n    if not all(var in dframe.columns for var in feature_vars):\n        raise ValueError(\"Not all 'feature_vars' are columns in 'dframe'\")\n    if dframe.empty:\n        raise ValueError(\"Input 'dframe' is empty\")\n    \n    df_metrics = dframe.copy()\n    poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n    poly_features = poly.fit_transform(dframe[feature_vars])\n    poly_feature_names = poly.get_feature_names_out(feature_vars)\n    \n    poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_metrics.index)\n    poly_df = poly_df.drop(columns=feature_vars, errors='ignore')\n    \n    df_metrics = pd.concat([df_metrics, poly_df], axis=1)\n    \n    if not df_metrics.index.equals(dframe.index):\n        raise ValueError(\"Index misalignment after concatenation\")\n    \n    return df_metrics\n    \n# === Process training data === \ndf_train_lag = create_lag_features(df_train, features)\ndf_train_rolling = create_rolling_features(df_train_lag, features)\n\nrolling_columns = [col for col in df_train_rolling.columns if col not in df_train_lag.columns]\n\ndf_train_combined = pd.concat([df_train_lag, df_train_rolling[rolling_columns]], axis=1)\n\n# === Process test data === \ndf_test_lag = create_lag_features(df_test, features)\ndf_test_rolling = create_rolling_features(df_test_lag, features)\n\nrolling_columns = [col for col in df_test_rolling.columns if col not in df_test_lag.columns]\n    \ndf_test_combined = pd.concat([df_test_lag, df_test_rolling[rolling_columns]], axis=1)\n\n# === Process Seasonal and Periodic features ==== \ndf_train_combined = create_periodic_features(df_train_combined)\ndf_test_combined = create_periodic_features(df_test_combined)\n\ndf_train_combined = multivariate_features(df_train_combined, features)\ndf_test_combined = multivariate_features(df_test_combined, features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:27:11.682141Z","iopub.execute_input":"2025-09-21T12:27:11.682407Z","iopub.status.idle":"2025-09-21T12:27:19.247033Z","shell.execute_reply.started":"2025-09-21T12:27:11.682381Z","shell.execute_reply":"2025-09-21T12:27:19.245728Z"}},"outputs":[{"name":"stdout","text":"NaNs in lagged columns before ffill: 154\nNaNs in lagged columns after ffill/bfill: 0\nNaNs in rolling columns before ffill: 1120028\nNaNs in rolling columns after ffill/bfill: 1120000\nNaNs in lagged columns before ffill: 154\nNaNs in lagged columns after ffill/bfill: 0\nNaNs in rolling columns before ffill: 223972\nNaNs in rolling columns after ffill/bfill: 223944\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"selected_features = ['A B K', 'G J', 'G H J', 'G', 'G H M', 'A B F', 'A I K', 'C G H', \n                      'A F K', 'K', 'G M', 'C G', 'G H', 'A B', 'A K', 'B D K', 'D', \n                      'E G M', 'A D F', 'A D K']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:27:19.249273Z","iopub.execute_input":"2025-09-21T12:27:19.249673Z","iopub.status.idle":"2025-09-21T12:27:19.255126Z","shell.execute_reply.started":"2025-09-21T12:27:19.249634Z","shell.execute_reply":"2025-09-21T12:27:19.253943Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Hyperpatameter optimization","metadata":{}},{"cell_type":"code","source":"import optuna\n\ndef objective(trial):\n    # Define hyperparameter search space\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 60, 70),\n        'max_depth': trial.suggest_int('max_depth', 8, 12),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n        'random_state': 42\n    }\n    \n    # Initialize model\n    model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', **params))\n    \n    # TimeSeriesSplit for cross-validation\n    tscv = TimeSeriesSplit(n_splits=5, test_size=10)\n    r2_scores = []\n\n    # Preparing data\n    X_train = df_train_combined[selected_features] \n    y_train = df_train_combined[targets]\n    \n    # Perform cross-validation\n    for train_idx, test_idx in tscv.split(X_train):\n        X_train_tmp, X_test_tmp = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_train_tmp, y_test_tmp = y_train.iloc[train_idx], y_train.iloc[test_idx]\n        \n        # Fit model\n        model.fit(X_train_tmp, y_train_tmp)\n        \n        # Predict and calculate R²\n        y_pred = model.predict(X_test_tmp)\n        r2 = r2_score(y_test_tmp, y_pred, multioutput='uniform_average')\n        r2_scores.append(r2)\n    \n    # Return average R² across folds (maximize)\n    return np.mean(r2_scores)\n\n# Create Optuna study and optimize\nstudy = optuna.create_study(direction='maximize')  # Maximize R²\nstudy.optimize(objective, n_trials=50)  # Adjust n_trials as needed\n\n# Print best parameters and value\nprint(\"Best Parameters:\", study.best_params)\nprint(\"Best R² Score:\", study.best_value)\n\n# Train final model with best parameters\nbest_params = study.best_params\nfinal_model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', **best_params))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:27:19.256212Z","iopub.execute_input":"2025-09-21T12:27:19.256498Z","iopub.status.idle":"2025-09-21T12:52:22.246723Z","shell.execute_reply.started":"2025-09-21T12:27:19.256472Z","shell.execute_reply":"2025-09-21T12:52:22.246013Z"}},"outputs":[{"name":"stderr","text":"[I 2025-09-21 12:27:19,280] A new study created in memory with name: no-name-72e45767-faa8-424e-a732-4e0aa957257c\n[I 2025-09-21 12:27:49,614] Trial 0 finished with value: 0.2898385333323804 and parameters: {'n_estimators': 59, 'max_depth': 10, 'learning_rate': 0.029711869646755013}. Best is trial 0 with value: 0.2898385333323804.\n[I 2025-09-21 12:28:17,040] Trial 1 finished with value: 0.23932836745444983 and parameters: {'n_estimators': 80, 'max_depth': 9, 'learning_rate': 0.029957112844895974}. Best is trial 0 with value: 0.2898385333323804.\n[I 2025-09-21 12:28:37,791] Trial 2 finished with value: 0.2604203954814598 and parameters: {'n_estimators': 60, 'max_depth': 9, 'learning_rate': 0.035209607408044116}. Best is trial 0 with value: 0.2898385333323804.\n[I 2025-09-21 12:29:18,930] Trial 3 finished with value: 0.274188378174761 and parameters: {'n_estimators': 82, 'max_depth': 10, 'learning_rate': 0.007753762634550341}. Best is trial 0 with value: 0.2898385333323804.\n[I 2025-09-21 12:30:49,677] Trial 4 finished with value: 0.30483147471252237 and parameters: {'n_estimators': 80, 'max_depth': 12, 'learning_rate': 0.016208705983170972}. Best is trial 4 with value: 0.30483147471252237.\n[I 2025-09-21 12:31:14,838] Trial 5 finished with value: 0.25072236537908166 and parameters: {'n_estimators': 76, 'max_depth': 9, 'learning_rate': 0.007427587679511433}. Best is trial 4 with value: 0.30483147471252237.\n[I 2025-09-21 12:31:47,150] Trial 6 finished with value: 0.33011127543231733 and parameters: {'n_estimators': 63, 'max_depth': 10, 'learning_rate': 0.015083965093957033}. Best is trial 6 with value: 0.33011127543231733.\n[I 2025-09-21 12:32:06,485] Trial 7 finished with value: 0.22410733897412238 and parameters: {'n_estimators': 56, 'max_depth': 9, 'learning_rate': 0.009040365158728697}. Best is trial 6 with value: 0.33011127543231733.\n[I 2025-09-21 12:32:26,965] Trial 8 finished with value: 0.3302974066763146 and parameters: {'n_estimators': 59, 'max_depth': 9, 'learning_rate': 0.019646954442152254}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:32:57,162] Trial 9 finished with value: 0.2659230020515053 and parameters: {'n_estimators': 58, 'max_depth': 10, 'learning_rate': 0.03576386967259112}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:33:12,367] Trial 10 finished with value: 0.32633028386953755 and parameters: {'n_estimators': 67, 'max_depth': 8, 'learning_rate': 0.017454311606683977}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:34:02,571] Trial 11 finished with value: 0.2937721357219158 and parameters: {'n_estimators': 67, 'max_depth': 11, 'learning_rate': 0.011224297535182873}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:34:17,234] Trial 12 finished with value: 0.31649182725282554 and parameters: {'n_estimators': 64, 'max_depth': 8, 'learning_rate': 0.020822781812579443}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:35:09,957] Trial 13 finished with value: 0.1282382470604177 and parameters: {'n_estimators': 72, 'max_depth': 11, 'learning_rate': 0.005040029686136732}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:35:57,864] Trial 14 finished with value: 0.30308909165845604 and parameters: {'n_estimators': 63, 'max_depth': 11, 'learning_rate': 0.022947361359772617}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:36:10,958] Trial 15 finished with value: 0.2916472287853401 and parameters: {'n_estimators': 55, 'max_depth': 8, 'learning_rate': 0.012777516023036584}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:37:21,086] Trial 16 finished with value: 0.19681479526356357 and parameters: {'n_estimators': 63, 'max_depth': 12, 'learning_rate': 0.04818827373675817}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:37:45,321] Trial 17 finished with value: 0.32624152508254023 and parameters: {'n_estimators': 71, 'max_depth': 9, 'learning_rate': 0.012798878233279508}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:38:20,406] Trial 18 finished with value: 0.3267869737887588 and parameters: {'n_estimators': 67, 'max_depth': 10, 'learning_rate': 0.020727130563361902}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:39:06,892] Trial 19 finished with value: 0.3096645302269021 and parameters: {'n_estimators': 61, 'max_depth': 11, 'learning_rate': 0.013933101116038432}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:39:45,217] Trial 20 finished with value: 0.30099624654313034 and parameters: {'n_estimators': 75, 'max_depth': 10, 'learning_rate': 0.009830298011871205}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:40:20,042] Trial 21 finished with value: 0.32801464509303946 and parameters: {'n_estimators': 66, 'max_depth': 10, 'learning_rate': 0.020996602737968512}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:40:54,235] Trial 22 finished with value: 0.2977330923591729 and parameters: {'n_estimators': 65, 'max_depth': 10, 'learning_rate': 0.02494939734232104}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:41:18,258] Trial 23 finished with value: 0.3272005187536228 and parameters: {'n_estimators': 69, 'max_depth': 9, 'learning_rate': 0.01794302254843791}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:42:04,155] Trial 24 finished with value: 0.31542851871904914 and parameters: {'n_estimators': 61, 'max_depth': 11, 'learning_rate': 0.015442739148364474}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:42:34,204] Trial 25 finished with value: 0.30019854252096023 and parameters: {'n_estimators': 57, 'max_depth': 10, 'learning_rate': 0.02818668315382803}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:42:56,965] Trial 26 finished with value: 0.3242871268040634 and parameters: {'n_estimators': 65, 'max_depth': 9, 'learning_rate': 0.019543084077334426}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:43:29,248] Trial 27 finished with value: 0.311206910145967 and parameters: {'n_estimators': 62, 'max_depth': 10, 'learning_rate': 0.025233917681629213}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:43:45,123] Trial 28 finished with value: 0.20138131530066666 and parameters: {'n_estimators': 74, 'max_depth': 8, 'learning_rate': 0.04527605544324439}. Best is trial 8 with value: 0.3302974066763146.\n[I 2025-09-21 12:44:09,385] Trial 29 finished with value: 0.33342988937815105 and parameters: {'n_estimators': 69, 'max_depth': 9, 'learning_rate': 0.0151306991585448}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:44:30,229] Trial 30 finished with value: 0.31928582522604043 and parameters: {'n_estimators': 59, 'max_depth': 9, 'learning_rate': 0.01443343734761096}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:44:54,166] Trial 31 finished with value: 0.31165348983017405 and parameters: {'n_estimators': 69, 'max_depth': 9, 'learning_rate': 0.011635358483587984}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:45:29,728] Trial 32 finished with value: 0.3243099475928619 and parameters: {'n_estimators': 68, 'max_depth': 10, 'learning_rate': 0.018259883984457093}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:45:54,574] Trial 33 finished with value: 0.24611292523044997 and parameters: {'n_estimators': 73, 'max_depth': 9, 'learning_rate': 0.031884481350367956}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:46:18,448] Trial 34 finished with value: 0.3306506312249665 and parameters: {'n_estimators': 65, 'max_depth': 9, 'learning_rate': 0.015782884160525805}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:46:32,492] Trial 35 finished with value: 0.32509264364844487 and parameters: {'n_estimators': 60, 'max_depth': 8, 'learning_rate': 0.015743318889878996}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:47:01,223] Trial 36 finished with value: 0.3216560781395671 and parameters: {'n_estimators': 85, 'max_depth': 9, 'learning_rate': 0.010316303616114379}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:47:17,355] Trial 37 finished with value: 0.26377445924171894 and parameters: {'n_estimators': 70, 'max_depth': 8, 'learning_rate': 0.008695303974879639}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:47:43,557] Trial 38 finished with value: 0.3321063887522513 and parameters: {'n_estimators': 77, 'max_depth': 9, 'learning_rate': 0.013128433738419003}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:48:09,611] Trial 39 finished with value: 0.24509301327828265 and parameters: {'n_estimators': 77, 'max_depth': 9, 'learning_rate': 0.007142142417544707}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:48:35,917] Trial 40 finished with value: 0.3322942887922222 and parameters: {'n_estimators': 79, 'max_depth': 9, 'learning_rate': 0.013225602447660778}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:49:02,593] Trial 41 finished with value: 0.3316387104474191 and parameters: {'n_estimators': 79, 'max_depth': 9, 'learning_rate': 0.012758947025578166}. Best is trial 29 with value: 0.33342988937815105.\n[I 2025-09-21 12:49:29,555] Trial 42 finished with value: 0.33362248002164074 and parameters: {'n_estimators': 79, 'max_depth': 9, 'learning_rate': 0.013125447012695931}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:49:56,118] Trial 43 finished with value: 0.33188756973732053 and parameters: {'n_estimators': 79, 'max_depth': 9, 'learning_rate': 0.012514782839689735}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:50:15,033] Trial 44 finished with value: 0.3227180758566496 and parameters: {'n_estimators': 82, 'max_depth': 8, 'learning_rate': 0.011236775278937027}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:50:41,856] Trial 45 finished with value: 0.2776400751961492 and parameters: {'n_estimators': 78, 'max_depth': 9, 'learning_rate': 0.008194476566353666}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:51:09,664] Trial 46 finished with value: 0.24239182799210307 and parameters: {'n_estimators': 82, 'max_depth': 9, 'learning_rate': 0.006629224868106257}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:51:28,193] Trial 47 finished with value: 0.30558318915410976 and parameters: {'n_estimators': 80, 'max_depth': 8, 'learning_rate': 0.009644412324891688}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:51:56,539] Trial 48 finished with value: 0.3303498041499857 and parameters: {'n_estimators': 84, 'max_depth': 9, 'learning_rate': 0.014038097108730576}. Best is trial 42 with value: 0.33362248002164074.\n[I 2025-09-21 12:52:22,240] Trial 49 finished with value: 0.32675656881916265 and parameters: {'n_estimators': 76, 'max_depth': 9, 'learning_rate': 0.012050858974062271}. Best is trial 42 with value: 0.33362248002164074.\n","output_type":"stream"},{"name":"stdout","text":"Best Parameters: {'n_estimators': 79, 'max_depth': 9, 'learning_rate': 0.013125447012695931}\nBest R² Score: 0.33362248002164074\n","output_type":"stream"}],"execution_count":6}]}