{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13114900,"sourceType":"datasetVersion","datasetId":8307798},{"sourceId":13115807,"sourceType":"datasetVersion","datasetId":8308421}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:40:07.919498Z","iopub.execute_input":"2025-09-22T14:40:07.920516Z","iopub.status.idle":"2025-09-22T14:40:07.943446Z","shell.execute_reply.started":"2025-09-22T14:40:07.920479Z","shell.execute_reply":"2025-09-22T14:40:07.942531Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/validation/test.csv\n/kaggle/input/training/train (1).csv\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load file paths\nfile_path_train = \"/kaggle/input/training/train (1).csv\"\nfile_path_test = \"/kaggle/input/validation/test.csv\"\n\n# Load training data\ntry:\n    df_train = pd.read_csv(file_path_train)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"File not found at {file_path_train}\")\n\n# Load test data\ntry:\n    df_test = pd.read_csv(file_path_test)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"File not found at {file_path_test}\")\n\n# Define features and targets\nfeatures = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\"]\ntargets = [\"Y1\", \"Y2\"]\n\n# Verify columns exist in training data\nrequired_columns = features + targets\nmissing_cols_train = [col for col in required_columns if col not in df_train.columns]\nif missing_cols_train:\n    raise ValueError(f\"Missing columns in training dataset: {missing_cols_train}\")\n\n# Verify columns in test data (allow missing targets)\nmissing_cols_test = [col for col in features if col not in df_test.columns]\nif missing_cols_test:\n    raise ValueError(f\"Missing columns in test dataset: {missing_cols_test}\")\n\n# Check data types\nprint(\"Training DataFrame dtypes:\\n\", df_train.dtypes)\nprint(\"Test DataFrame dtypes:\\n\", df_test.dtypes)\nif not all(df_train[features].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in training features: {df_train[features].dtypes}\")\nif not all(df_test[features].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in test features: {df_test[features].dtypes}\")\nif all(col in df_train.columns for col in targets) and not all(df_train[targets].dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n    raise ValueError(f\"Non-numeric data in training targets: {df_train[targets].dtypes}\")\n\n# Check for NaNs in original datasets\nprint(\"NaNs in original training DataFrame:\", df_train.isna().sum().sum())\nprint(\"NaN counts per column in training DataFrame:\\n\", df_train.isna().sum())\nprint(\"NaNs in original test DataFrame:\", df_test.isna().sum().sum())\nprint(\"NaN counts per column in test DataFrame:\\n\", df_test.isna().sum())\n\n# Forward fill and backward fill NaNs in original datasets\ndf_train = df_train[features + targets].ffill().bfill()\ndf_test = df_test[features + (targets if all(col in df_test.columns for col in targets) else [])].ffill().bfill()\nprint(\"NaNs after ffill/bfill in training DataFrame:\", df_train.isna().sum().sum())\nprint(\"NaNs after ffill/bfill in test DataFrame:\", df_test.isna().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:40:07.945240Z","iopub.execute_input":"2025-09-22T14:40:07.945507Z","iopub.status.idle":"2025-09-22T14:40:08.403241Z","shell.execute_reply.started":"2025-09-22T14:40:07.945485Z","shell.execute_reply":"2025-09-22T14:40:08.402288Z"}},"outputs":[{"name":"stdout","text":"Training DataFrame dtypes:\n time      int64\nA       float64\nB       float64\nC       float64\nD       float64\nE       float64\nF       float64\nG       float64\nH       float64\nI       float64\nJ       float64\nK       float64\nL       float64\nM       float64\nN       float64\nY1      float64\nY2      float64\ndtype: object\nTest DataFrame dtypes:\n id        int64\ntime      int64\nA       float64\nB       float64\nC       float64\nD       float64\nE       float64\nF       float64\nG       float64\nH       float64\nI       float64\nJ       float64\nK       float64\nL       float64\nM       float64\nN       float64\ndtype: object\nNaNs in original training DataFrame: 0\nNaN counts per column in training DataFrame:\n time    0\nA       0\nB       0\nC       0\nD       0\nE       0\nF       0\nG       0\nH       0\nI       0\nJ       0\nK       0\nL       0\nM       0\nN       0\nY1      0\nY2      0\ndtype: int64\nNaNs in original test DataFrame: 0\nNaN counts per column in test DataFrame:\n id      0\ntime    0\nA       0\nB       0\nC       0\nD       0\nE       0\nF       0\nG       0\nH       0\nI       0\nJ       0\nK       0\nL       0\nM       0\nN       0\ndtype: int64\nNaNs after ffill/bfill in training DataFrame: 0\nNaNs after ffill/bfill in test DataFrame: 0\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# 1. Lag and Shift Features\ndef create_lag_features(dframe, feature_vars, lags=[1, 3, 7]):\n    df_lagged = dframe.copy()\n    new_columns = {}\n    for var in feature_vars:\n        for lag in lags:\n            new_columns[f'{var}_lag{lag}'] = df_lagged[var].shift(lag)\n    new_columns_df = pd.DataFrame(new_columns, index=df_lagged.index)\n    df_lagged = pd.concat([df_lagged, new_columns_df], axis=1)\n    lag_columns = list(new_columns.keys())\n    print(\"NaNs in lagged columns before ffill:\", df_lagged[lag_columns].isna().sum().sum())\n    df_lagged[lag_columns] = df_lagged[lag_columns].ffill().bfill()\n    print(\"NaNs in lagged columns after ffill/bfill:\", df_lagged[lag_columns].isna().sum().sum())\n    return df_lagged\n\n# 2. Rolling Window Statistics\ndef create_rolling_features(dframe, feature_vars, windows=[1, 3, 7], ema_span=7):\n    df_metrics = dframe.copy()\n    new_columns = {}\n    for var in feature_vars:\n        for window in windows:\n            new_columns[f'{var}_mean_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).mean()\n            new_columns[f'{var}_var_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).var().fillna(0)\n            new_columns[f'{var}_std_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).std()\n            new_columns[f'{var}_min_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).min()\n            new_columns[f'{var}_max_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).max()\n            new_columns[f'{var}_median_{window}d'] = df_metrics[var].rolling(window=window, min_periods=1).median()\n        new_columns[f'{var}_ema_{ema_span}d'] = df_metrics[var].ewm(span=ema_span, adjust=False).mean()\n    new_columns_df = pd.DataFrame(new_columns, index=df_metrics.index)\n    df_metrics = pd.concat([df_metrics, new_columns_df], axis=1)\n    rolling_columns = list(new_columns.keys())\n    print(\"NaNs in rolling columns before ffill:\", df_metrics[rolling_columns].isna().sum().sum())\n    df_metrics[rolling_columns] = df_metrics[rolling_columns].ffill().bfill()\n    print(\"NaNs in rolling columns after ffill/bfill:\", df_metrics[rolling_columns].isna().sum().sum())\n    return df_metrics\n\n# 3. Seasonal and Periodic features \ndef create_periodic_features(dframe, windows = [1, 3, 7]):\n    df_metrics = dframe.copy()\n    t = np.arange(len(dframe))\n    new_columns = {}\n    \n    for window in windows:\n        new_columns[f'sin_{window}d'] = np.sin(2 * np.pi * t / window)\n        new_columns[f'cos_{window}d'] = np.cos(2 * np.pi * t / window)\n    new_columns_df = pd.DataFrame(new_columns, index = dframe.index)\n    df_metrics = pd.concat([df_metrics, new_columns_df], axis = 1)\n\n    return df_metrics\n\n# 4. Multivariate features \ndef multivariate_features(dframe, feature_vars):\n\n    if not isinstance(dframe, pd.DataFrame):\n        raise ValueError(\"Input 'dframe' must be a pandas DataFrame\")\n    if not all(var in dframe.columns for var in feature_vars):\n        raise ValueError(\"Not all 'feature_vars' are columns in 'dframe'\")\n    if dframe.empty:\n        raise ValueError(\"Input 'dframe' is empty\")\n    \n    df_metrics = dframe.copy()\n    poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n    poly_features = poly.fit_transform(dframe[feature_vars])\n    poly_feature_names = poly.get_feature_names_out(feature_vars)\n    \n    poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_metrics.index)\n    poly_df = poly_df.drop(columns=feature_vars, errors='ignore')\n    \n    df_metrics = pd.concat([df_metrics, poly_df], axis=1)\n    \n    if not df_metrics.index.equals(dframe.index):\n        raise ValueError(\"Index misalignment after concatenation\")\n    \n    return df_metrics\n    \n# === Process training data === \ndf_train_lag = create_lag_features(df_train, features)\ndf_train_rolling = create_rolling_features(df_train_lag, features)\n\nrolling_columns = [col for col in df_train_rolling.columns if col not in df_train_lag.columns]\n\ndf_train_combined = pd.concat([df_train_lag, df_train_rolling[rolling_columns]], axis=1)\n\n# === Process test data === \ndf_test_lag = create_lag_features(df_test, features)\ndf_test_rolling = create_rolling_features(df_test_lag, features)\n\nrolling_columns = [col for col in df_test_rolling.columns if col not in df_test_lag.columns]\n    \ndf_test_combined = pd.concat([df_test_lag, df_test_rolling[rolling_columns]], axis=1)\n\n# === Process Seasonal and Periodic features ==== \ndf_train_combined = create_periodic_features(df_train_combined)\ndf_test_combined = create_periodic_features(df_test_combined)\n\ndf_train_combined = multivariate_features(df_train_combined, features)\ndf_test_combined = multivariate_features(df_test_combined, features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:40:08.404170Z","iopub.execute_input":"2025-09-22T14:40:08.404400Z","iopub.status.idle":"2025-09-22T14:40:14.297465Z","shell.execute_reply.started":"2025-09-22T14:40:08.404383Z","shell.execute_reply":"2025-09-22T14:40:14.296615Z"}},"outputs":[{"name":"stdout","text":"NaNs in lagged columns before ffill: 154\nNaNs in lagged columns after ffill/bfill: 0\nNaNs in rolling columns before ffill: 1120028\nNaNs in rolling columns after ffill/bfill: 1120000\nNaNs in lagged columns before ffill: 154\nNaNs in lagged columns after ffill/bfill: 0\nNaNs in rolling columns before ffill: 223972\nNaNs in rolling columns after ffill/bfill: 223944\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Selected Features","metadata":{}},{"cell_type":"code","source":"selected_features = ['A B K', 'G J', 'G H J', 'G', 'G H M', 'A B F', 'A I K', 'C G H', \n                      'A F K', 'K', 'G M', 'C G', 'G H', 'A B', 'A K', 'B D K', 'D', \n                      'E G M', 'A D F', 'A D K']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:40:14.299214Z","iopub.execute_input":"2025-09-22T14:40:14.299587Z","iopub.status.idle":"2025-09-22T14:40:14.304388Z","shell.execute_reply.started":"2025-09-22T14:40:14.299561Z","shell.execute_reply":"2025-09-22T14:40:14.303359Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import Ridge\nfrom xgboost import XGBRegressor\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Prepare data\nX_train = df_train_combined[selected_features]\ny_train = df_train_combined[targets]\nX_test = df_test_combined[selected_features]\n\n# Time Series Split (confirm test_size; assuming 7 days if daily data)\ntscv = TimeSeriesSplit(n_splits=5, test_size=7)  # Adjusted to 7 days\n\n# Define single XGBoost model wrapped in MultiOutputRegressor\nbase_xgb = XGBRegressor(\n    objective=\"reg:squarederror\",\n    n_estimators=66,\n    max_depth=9,\n    learning_rate=0.015585283417938404,\n    subsample=0.819811327961394,\n    colsample_bytree=0.7888058310373385,\n    min_child_weight=8,\n    random_state=42\n)\n\nmodel = MultiOutputRegressor(base_xgb)\n\n# Initialize arrays for OOF predictions and true values\nn_targets = len(targets)\noof_xgb = np.zeros((len(y_train), n_targets))\noof_sarima = np.zeros((len(y_train), n_targets))\noof_y = np.zeros((len(y_train), n_targets))\nr2_scores = {f'target_{i}': [] for i in range(n_targets)}\n\n# Check for log transformation\nis_log_transformed = any('log' in target for target in targets)\n\nfold = 0\nfor train_idx, val_idx in tscv.split(X_train):\n    fold += 1\n    print(f\"Fold {fold}\")\n\n    # Split data\n    X_train_tmp, y_train_tmp = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_val_tmp, y_val_tmp = X_train.iloc[val_idx], y_train.iloc[val_idx]\n\n    # Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train_tmp)\n    X_val_scaled = scaler.transform(X_val_tmp)\n    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_tmp.columns, index=X_train_tmp.index)\n    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val_tmp.columns, index=X_val_tmp.index)\n\n    # Fit SARIMA and store OOF predictions\n    for t_idx, target in enumerate(targets):\n        sarima_model = SARIMAX(y_train_tmp[target], order=(1, 1, 1), seasonal_order=(1, 0, 1, 7))\n        sarima_fit = sarima_model.fit(disp=False)\n        sarima_pred_val = sarima_fit.forecast(steps=len(val_idx))\n        oof_sarima[val_idx, t_idx] = sarima_pred_val\n        oof_y[val_idx, t_idx] = y_val_tmp[target]\n\n    # Fit XGBoost and store OOF predictions\n    model.fit(X_train_scaled, y_train_tmp)\n    xgb_pred_val = model.predict(X_val_scaled)\n    oof_xgb[val_idx, :] = xgb_pred_val\n\n    # Meta-learner (Ridge) for each target using OOF predictions\n    for t_idx, target in enumerate(targets):\n        # Use OOF predictions from previous folds (if available) or current fold’s validation\n        valid_idx = np.where(oof_xgb[:, t_idx] != 0)[0]\n        if len(valid_idx) == 0:\n            print(f\"Warning: No OOF predictions for {target} in fold {fold}. Skipping meta-learner.\")\n            continue\n\n        X_meta_train_tmp = np.column_stack([oof_xgb[valid_idx, t_idx], oof_sarima[valid_idx, t_idx]])\n        y_meta_train_tmp = oof_y[valid_idx, t_idx]\n        X_meta_val = np.column_stack([xgb_pred_val[:, t_idx], sarima_pred_val])\n\n        # Fit meta-learner\n        meta_model = Ridge(alpha=1.0)\n        meta_model.fit(X_meta_train_tmp, y_meta_train_tmp)\n        y_pred_val = meta_model.predict(X_meta_val)\n\n        # Calculate R² (on original scale if log-transformed)\n        if is_log_transformed:\n            y_pred_val = np.expm1(y_pred_val)\n            y_val_tmp_r2 = np.expm1(y_val_tmp[target])\n        else:\n            y_val_tmp_r2 = y_val_tmp[target]\n\n        r2 = r2_score(y_val_tmp_r2, y_pred_val)\n        r2_scores[f'target_{t_idx}'].append(r2)\n        print(f\"Fold R² for {target}: {r2:.4f}\")\n\n# Average CV R² per target\nfor t_idx, target in enumerate(targets):\n    avg_r2 = np.mean(r2_scores[f'target_{t_idx}'])\n    print(f\"Average CV R² for {target}: {avg_r2:.4f}\")\n\n# === Final model training and test predictions ===\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n\n# Fit XGBoost on full training data\nmodel.fit(X_train_scaled, y_train)\nxgb_predictions_test = model.predict(X_test_scaled)\n\n# Fit SARIMA on full training data for each target\nsarima_predictions_test = np.zeros((len(X_test), n_targets))\nfor t_idx, target in enumerate(targets):\n    sarima_model = SARIMAX(y_train[target], order=(1, 1, 1), seasonal_order=(1, 0, 1, 7))\n    sarima_fit = sarima_model.fit(disp=False)\n    sarima_predictions_test[:, t_idx] = sarima_fit.forecast(steps=len(X_test))\n\n# Meta-learner predictions\nfinal_predictions = np.zeros((len(X_test), n_targets))\nmeta_models = []  # Store meta-learners for debugging\n\nfor t_idx, target in enumerate(targets):\n    # Use all OOF predictions (non-zero)\n    valid_idx = np.where(oof_xgb[:, t_idx] != 0)[0]\n    if len(valid_idx) == 0:\n        print(f\"Warning: No valid OOF predictions for {target}. Using XGBoost predictions.\")\n        final_predictions[:, t_idx] = xgb_predictions_test[:, t_idx]  # Fallback to XGBoost\n        continue\n\n    X_meta_oof = np.column_stack([oof_xgb[valid_idx, t_idx], oof_sarima[valid_idx, t_idx]])\n    y_meta_oof = oof_y[valid_idx, t_idx]\n\n    # Fit meta-learner on OOF data\n    meta_model = Ridge(alpha=1.0)\n    meta_model.fit(X_meta_oof, y_meta_oof)\n    meta_models.append(meta_model)\n\n    # Combine test predictions\n    X_meta_test = np.column_stack([xgb_predictions_test[:, t_idx], sarima_predictions_test[:, t_idx]])\n    final_predictions[:, t_idx] = meta_model.predict(X_meta_test)\n\n    # Inverse log transform if applied\n    if is_log_transformed:\n        final_predictions[:, t_idx] = np.expm1(final_predictions[:, t_idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:40:14.305370Z","iopub.execute_input":"2025-09-22T14:40:14.305612Z","iopub.status.idle":"2025-09-22T14:52:02.769882Z","shell.execute_reply.started":"2025-09-22T14:40:14.305593Z","shell.execute_reply":"2025-09-22T14:52:02.768722Z"}},"outputs":[{"name":"stdout","text":"Fold 1\nFold R² for Y1: 0.8725\nFold R² for Y2: 0.4475\nFold 2\nFold R² for Y1: 0.9393\nFold R² for Y2: 0.5823\nFold 3\nFold R² for Y1: 0.8330\nFold R² for Y2: 0.5546\nFold 4\nFold R² for Y1: 0.9486\nFold R² for Y2: -1.6456\nFold 5\nFold R² for Y1: 0.6607\nFold R² for Y2: -2.1035\nAverage CV R² for Y1: 0.8508\nAverage CV R² for Y2: -0.4330\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Final prediction submission","metadata":{}},{"cell_type":"code","source":"id_list = np.arange(1, len(X_test) + 1, 1)\n\nfinal_predictions_df = pd.DataFrame({\n    'id': id_list,`z\n    'Y1': final_predictions[:, 0],  \n    'Y2': final_predictions[:, 1]   \n}, index=X_test.index)\n\nprint(\"\\nFinal test predictions:\")\nprint(final_predictions_df)\n\nfinal_predictions_df.to_csv(\"predictions_final.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:52:02.771417Z","iopub.execute_input":"2025-09-22T14:52:02.771850Z","iopub.status.idle":"2025-09-22T14:52:02.860701Z","shell.execute_reply.started":"2025-09-22T14:52:02.771814Z","shell.execute_reply":"2025-09-22T14:52:02.859701Z"}},"outputs":[{"name":"stdout","text":"\nFinal test predictions:\n          id        Y1        Y2\n0          1  0.442238 -0.037126\n1          2 -0.123280 -0.192775\n2          3 -0.116707  0.020937\n3          4 -0.399684  0.194474\n4          5 -1.103073  0.296873\n...      ...       ...       ...\n15991  15992 -0.297535 -0.029059\n15992  15993 -0.119928 -0.193963\n15993  15994  0.274811 -0.114279\n15994  15995  0.440322 -0.100829\n15995  15996  0.294222 -0.158843\n\n[15996 rows x 3 columns]\n","output_type":"stream"}],"execution_count":23}]}